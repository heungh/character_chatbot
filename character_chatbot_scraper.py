#!/usr/bin/env python3
"""
ë‚˜ë¬´ìœ„í‚¤ ìºë¦­í„° ì •ë³´ ìë™ ì¶”ì¶œê¸°
"""

import requests
from bs4 import BeautifulSoup
import re
import json
from typing import Dict, Any, Optional
import time
from urllib.parse import quote, urljoin, urlparse
import streamlit as st
import boto3
import os
from pathlib import Path
import hashlib

class NamuWikiScraper:
    def __init__(self):
        self.base_url = "https://namu.wiki"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.bedrock_client = boto3.client(
            "bedrock-runtime", 
            region_name="us-east-1"
        )
        
        # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì´ë¯¸ì§€ ì €ì¥ìš©)
        self.s3_client = boto3.client("s3", region_name="us-east-1")
        try:
            with open(Path(__file__).parent / "admin_config.json", "r", encoding="utf-8") as f:
                _cfg = json.load(f)
                self.bucket_name = _cfg.get("bucket_name", "")
        except (FileNotFoundError, json.JSONDecodeError):
            self.bucket_name = os.environ.get("S3_BUCKET_NAME", "")
    
    def search_character(self, character_name: str) -> Optional[str]:
        """ìºë¦­í„° ì´ë¦„ìœ¼ë¡œ ë‚˜ë¬´ìœ„í‚¤ í˜ì´ì§€ URL ì°¾ê¸°"""
        try:
            # ë‚˜ë¬´ìœ„í‚¤ ê²€ìƒ‰ URL
            search_url = f"{self.base_url}/go/{quote(character_name)}"
            
            response = self.session.get(search_url, timeout=10)
            if response.status_code == 200:
                return response.url
            
            # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ì²« ë²ˆì§¸ ê²°ê³¼ ì°¾ê¸°
            search_results_url = f"{self.base_url}/Search?q={quote(character_name)}"
            response = self.session.get(search_results_url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                # ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ ë§í¬ ì°¾ê¸°
                first_result = soup.find('a', {'class': 'title'})
                if first_result and first_result.get('href'):
                    return urljoin(self.base_url, first_result['href'])
            
            return None
            
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    def extract_character_info(self, page_url: str) -> Dict[str, Any]:
        """ë‚˜ë¬´ìœ„í‚¤ í˜ì´ì§€ì—ì„œ ìºë¦­í„° ì •ë³´ ì¶”ì¶œ"""
        try:
            response = self.session.get(page_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í˜ì´ì§€ ì œëª©
            title_element = soup.find('h1', {'class': 'title'}) or soup.find('title')
            title = title_element.get_text().strip() if title_element else "ì•Œ ìˆ˜ ì—†ìŒ"
            
            # ì •ë³´ ì¶”ì¶œ
            character_info = {
                "name": self._clean_title(title),
                "role": "",
                "personality": "",
                "abilities": [],
                "hobbies": [],
                "background": "",
                "catchphrase": "",
                "speaking_style": "",
                "additional_info": {}
            }
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_div = soup.find('div', {'class': 'wiki-content'}) or soup.find('div', {'id': 'content'})
            if not content_div:
                content_div = soup.find('div', {'class': 'document'})
            
            if content_div:
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
                text_content = content_div.get_text()
                
                # ì •ë³´ ë°•ìŠ¤ì—ì„œ ì •ë³´ ì¶”ì¶œ
                self._extract_from_infobox(soup, character_info)
                
                # ë³¸ë¬¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                self._extract_from_content(text_content, character_info)
                
                # ì„¹ì…˜ë³„ ì •ë³´ ì¶”ì¶œ
                self._extract_sections(soup, character_info)
            
            return character_info
            
        except Exception as e:
            st.error(f"í˜ì´ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    def _clean_title(self, title: str) -> str:
        """ì œëª© ì •ë¦¬"""
        # ë‚˜ë¬´ìœ„í‚¤ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        title = re.sub(r'\s*-\s*ë‚˜ë¬´ìœ„í‚¤.*', '', title)
        title = re.sub(r'\([^)]*\)', '', title)  # ê´„í˜¸ ë‚´ìš© ì œê±°
        return title.strip()
    
    def _extract_from_infobox(self, soup: BeautifulSoup, character_info: Dict[str, Any]):
        """ì •ë³´ ë°•ìŠ¤ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì •ë³´ ë°•ìŠ¤ ì°¾ê¸°
            infobox = soup.find('table', {'class': 'infobox'}) or soup.find('div', {'class': 'infobox'})
            
            if infobox:
                rows = infobox.find_all('tr')
                for row in rows:
                    cells = row.find_all(['th', 'td'])
                    if len(cells) >= 2:
                        key = cells[0].get_text().strip()
                        value = cells[1].get_text().strip()
                        
                        # í‚¤ì›Œë“œ ë§¤ì¹­
                        if any(keyword in key.lower() for keyword in ['ì§ì—…', 'ì—­í• ', 'í¬ì§€ì…˜']):
                            character_info['role'] = value
                        elif any(keyword in key.lower() for keyword in ['ì„±ê²©', 'íŠ¹ì§•']):
                            character_info['personality'] = value
                        elif any(keyword in key.lower() for keyword in ['ëŠ¥ë ¥', 'ìŠ¤í‚¬', 'ê¸°ìˆ ']):
                            character_info['abilities'] = [ability.strip() for ability in value.split(',')]
                        elif any(keyword in key.lower() for keyword in ['ì·¨ë¯¸', 'ì¢‹ì•„í•˜ëŠ” ê²ƒ']):
                            character_info['hobbies'] = [hobby.strip() for hobby in value.split(',')]
                        else:
                            character_info['additional_info'][key] = value
        except Exception:
            pass
    
    def _extract_from_content(self, text: str, character_info: Dict[str, Any]):
        """ë³¸ë¬¸ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì„±ê²© ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
            personality_patterns = [
                r'ì„±ê²©.*?(?:ì´ë‹¤|í•˜ë‹¤|ìˆë‹¤)\.?',
                r'.*?í•œ ì„±ê²©.*?\.?',
                r'.*?ì„±í–¥.*?\.?'
            ]
            
            for pattern in personality_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches and not character_info['personality']:
                    character_info['personality'] = matches[0][:200]  # 200ì ì œí•œ
                    break
            
            # ë°°ê²½ ìŠ¤í† ë¦¬ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¬¸ë‹¨)
            paragraphs = text.split('\n\n')
            if paragraphs and not character_info['background']:
                first_paragraph = paragraphs[0].strip()
                if len(first_paragraph) > 50:  # ì¶©ë¶„íˆ ê¸´ ë¬¸ë‹¨ë§Œ
                    character_info['background'] = first_paragraph[:300]  # 300ì ì œí•œ
            
            # ëŠ¥ë ¥ ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
            ability_keywords = ['ëŠ¥ë ¥', 'ìŠ¤í‚¬', 'ê¸°ìˆ ', 'ë§ˆë²•', 'ì´ˆëŠ¥ë ¥', 'íŠ¹ìˆ˜ëŠ¥ë ¥']
            for keyword in ability_keywords:
                pattern = f'{keyword}.*?(?:ì´ë‹¤|í•˜ë‹¤|ìˆë‹¤)\.?'
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    abilities_text = ' '.join(matches)
                    # ì‰¼í‘œë‚˜ 'ê·¸ë¦¬ê³ 'ë¡œ ë¶„ë¦¬
                    abilities = re.split(r'[,ï¼Œã€]|ê·¸ë¦¬ê³ |ë˜í•œ', abilities_text)
                    character_info['abilities'].extend([
                        ability.strip() for ability in abilities 
                        if ability.strip() and len(ability.strip()) > 2
                    ])
                    break
            
        except Exception:
            pass
    
    def _extract_sections(self, soup: BeautifulSoup, character_info: Dict[str, Any]):
        """ì„¹ì…˜ë³„ ì •ë³´ ì¶”ì¶œ"""
        try:
            # í—¤ë”© íƒœê·¸ ì°¾ê¸°
            headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            
            for heading in headings:
                heading_text = heading.get_text().lower()
                
                # ì„±ê²© ì„¹ì…˜
                if any(keyword in heading_text for keyword in ['ì„±ê²©', 'íŠ¹ì§•', 'ê°œì„±']):
                    next_content = self._get_section_content(heading)
                    if next_content and not character_info['personality']:
                        character_info['personality'] = next_content[:200]
                
                # ëŠ¥ë ¥ ì„¹ì…˜
                elif any(keyword in heading_text for keyword in ['ëŠ¥ë ¥', 'ìŠ¤í‚¬', 'ê¸°ìˆ ', 'ì „íˆ¬']):
                    next_content = self._get_section_content(heading)
                    if next_content:
                        abilities = re.split(r'[,ï¼Œã€]|\n', next_content)
                        character_info['abilities'].extend([
                            ability.strip() for ability in abilities 
                            if ability.strip() and len(ability.strip()) > 2
                        ])
                
                # ë°°ê²½ ì„¹ì…˜
                elif any(keyword in heading_text for keyword in ['ë°°ê²½', 'ê³¼ê±°', 'ì—­ì‚¬', 'ìŠ¤í† ë¦¬']):
                    next_content = self._get_section_content(heading)
                    if next_content and not character_info['background']:
                        character_info['background'] = next_content[:300]
        except Exception:
            pass
    
    def refine_character_info_with_bedrock(self, raw_text: str, character_name: str) -> Dict[str, Any]:
        """Bedrock Claudeë¥¼ í™œìš©í•˜ì—¬ ì¶”ì¶œëœ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ìºë¦­í„° ì •ë³´ë¡œ ì •ì œ"""
        try:
            prompt = f"""
ë‹¤ìŒì€ ë‚˜ë¬´ìœ„í‚¤ì—ì„œ ì¶”ì¶œí•œ '{character_name}' ìºë¦­í„°ì— ëŒ€í•œ ì›ì‹œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì´ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ì›ì‹œ í…ìŠ¤íŠ¸:
{raw_text[:3000]}  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "name": "ìºë¦­í„°ì˜ ì •í™•í•œ ì´ë¦„",
    "role": "ìºë¦­í„°ì˜ ì—­í• ì´ë‚˜ ì§ì—… (50ì ì´ë‚´)",
    "personality": "ìºë¦­í„°ì˜ ì„±ê²© íŠ¹ì§•ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª… (150ì ì´ë‚´)",
    "abilities": ["ëŠ¥ë ¥1", "ëŠ¥ë ¥2", "ëŠ¥ë ¥3"],  // ìµœëŒ€ 5ê°œ
    "hobbies": ["ì·¨ë¯¸1", "ì·¨ë¯¸2"],  // ìµœëŒ€ 3ê°œ
    "background": "ìºë¦­í„°ì˜ ë°°ê²½ ìŠ¤í† ë¦¬ë¥¼ í¥ë¯¸ë¡­ê²Œ ìš”ì•½ (200ì ì´ë‚´)",
    "catchphrase": "ìºë¦­í„°ê°€ ìì£¼ í•˜ëŠ” ë§ì´ë‚˜ ëŒ€í‘œ ëŒ€ì‚¬",
    "speaking_style": "ìºë¦­í„°ì˜ ë§íˆ¬ë‚˜ ì–¸ì–´ íŠ¹ì§• ì„¤ëª… (100ì ì´ë‚´)"
}}

ê·œì¹™:
1. ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±
2. ìºë¦­í„°ì˜ ë§¤ë ¥ì„ ì‚´ë¦° ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´ ì‚¬ìš©
3. ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ìºë¦­í„°ì˜ íŠ¹ì„±ì— ë§ê²Œ ì°½ì˜ì ìœ¼ë¡œ ë³´ì™„
4. JSON í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”
5. ê° í•„ë“œëŠ” ì§€ì •ëœ ê¸€ì ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
"""

            body = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2000,
                "messages": [
                    {
                        "role": "user",
                        "content": [{"type": "text", "text": prompt}]
                    }
                ],
                "temperature": 0.3  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ì„¤ì •
            })

            # Claude 4.0 ëª¨ë¸ ì‹œë„, ì‹¤íŒ¨ì‹œ 3.5 Sonnet ì‚¬ìš©
            sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
            sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
            
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_4_model_id,
                    body=body
                )
            except Exception:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_3_7_model_id,
                    body=body
                )
            
            response_body = json.loads(response.get("body").read())
            refined_text = response_body.get("content", [{}])[0].get("text", "")
            
            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', refined_text, re.DOTALL)
            if json_match:
                refined_info = json.loads(json_match.group())
                return refined_info
            else:
                st.warning("Bedrock ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            st.warning(f"Bedrock ì •ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}. ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return None
    
    def extract_character_image(self, soup: BeautifulSoup, character_name: str) -> Optional[str]:
        """ë‚˜ë¬´ìœ„í‚¤ í˜ì´ì§€ì—ì„œ ìºë¦­í„° ëŒ€í‘œ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        try:
            # ì´ë¯¸ì§€ í›„ë³´ë“¤ì„ ì°¾ê¸° ìœ„í•œ ì—¬ëŸ¬ ë°©ë²• ì‹œë„
            image_candidates = []
            
            # 1. ì •ë³´ë°•ìŠ¤ ë‚´ ì´ë¯¸ì§€
            infobox = soup.find('table', {'class': 'infobox'}) or soup.find('div', {'class': 'infobox'})
            if infobox:
                infobox_imgs = infobox.find_all('img')
                for img in infobox_imgs:
                    src = img.get('src') or img.get('data-src')
                    if src and self._is_valid_character_image(src):
                        image_candidates.append(src)
            
            # 2. ì²« ë²ˆì§¸ ì„¹ì…˜ì˜ ì´ë¯¸ì§€ë“¤
            first_images = soup.find_all('img')[:10]  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸
            for img in first_images:
                src = img.get('src') or img.get('data-src')
                if src and self._is_valid_character_image(src):
                    image_candidates.append(src)
            
            # 3. ê°€ì¥ ì í•©í•œ ì´ë¯¸ì§€ ì„ íƒ
            if image_candidates:
                # ì¤‘ë³µ ì œê±°
                image_candidates = list(set(image_candidates))
                
                # ìš°ì„ ìˆœìœ„: í° ì´ë¯¸ì§€, ìºë¦­í„° ì´ë¦„ í¬í•¨, jpg/png í™•ì¥ì
                best_image = self._select_best_image(image_candidates, character_name)
                
                if best_image:
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if best_image.startswith('//'):
                        best_image = 'https:' + best_image
                    elif best_image.startswith('/'):
                        best_image = urljoin(self.base_url, best_image)
                    
                    return best_image
            
            return None
            
        except Exception as e:
            st.warning(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _is_valid_character_image(self, src: str) -> bool:
        """ìœ íš¨í•œ ìºë¦­í„° ì´ë¯¸ì§€ì¸ì§€ í™•ì¸"""
        if not src:
            return False
        
        # ì œì™¸í•  ì´ë¯¸ì§€ë“¤
        exclude_patterns = [
            'icon', 'logo', 'banner', 'button', 'arrow', 'edit',
            'commons', 'wikimedia', 'namu.wiki', 'advertisement'
        ]
        
        src_lower = src.lower()
        
        # ì œì™¸ íŒ¨í„´ í™•ì¸
        for pattern in exclude_patterns:
            if pattern in src_lower:
                return False
        
        # ì´ë¯¸ì§€ í™•ì¥ì í™•ì¸
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
        if not any(ext in src_lower for ext in valid_extensions):
            return False
        
        return True
    
    def _select_best_image(self, candidates: list, character_name: str) -> Optional[str]:
        """ì´ë¯¸ì§€ í›„ë³´ ì¤‘ ê°€ì¥ ì í•©í•œ ì´ë¯¸ì§€ ì„ íƒ"""
        if not candidates:
            return None
        
        scored_images = []
        
        for img_url in candidates:
            score = 0
            img_lower = img_url.lower()
            
            # ìºë¦­í„° ì´ë¦„ì´ í¬í•¨ëœ ê²½ìš° ë†’ì€ ì ìˆ˜
            if character_name.lower() in img_lower:
                score += 10
            
            # íŒŒì¼ í¬ê¸° ì¶”ì • (URLì— í¬ê¸° ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
            size_match = re.search(r'(\d+)x(\d+)', img_url)
            if size_match:
                width, height = int(size_match.group(1)), int(size_match.group(2))
                # ì ë‹¹í•œ í¬ê¸°ì˜ ì´ë¯¸ì§€ ì„ í˜¸ (ë„ˆë¬´ ì‘ê±°ë‚˜ í¬ì§€ ì•Šì€)
                if 100 <= width <= 800 and 100 <= height <= 800:
                    score += 5
                elif width >= 200 and height >= 200:
                    score += 3
            
            # í™•ì¥ìë³„ ì ìˆ˜
            if '.jpg' in img_lower or '.jpeg' in img_lower:
                score += 2
            elif '.png' in img_lower:
                score += 3
            elif '.webp' in img_lower:
                score += 1
            
            scored_images.append((score, img_url))
        
        # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_images.sort(key=lambda x: x[0], reverse=True)
        
        return scored_images[0][1] if scored_images else None
    
    def _korean_to_roman(self, text: str) -> str:
        """í•œê¸€ì„ ë¡œë§ˆìë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë§¤í•‘)"""
        # ê°„ë‹¨í•œ í•œê¸€ ë‹¨ì–´ ë§¤í•‘
        word_map = {
            'í”¼ì¹´ì¸„': 'pikachu',
            'ë‚˜ë£¨í† ': 'naruto', 
            'ì„¸ì¼ëŸ¬ë¬¸': 'sailormoon',
            'ì†ì˜¤ê³µ': 'songoku',
            'ë£¨í”¼': 'luffy',
            'ì´ì¹˜ê³ ': 'ichigo',
            'ë‚˜ì¸ ': 'natsu',
            'ë£¨ë¯¸': 'rumi',
            'ë¯¸ë¼': 'mira',
            'ì‚¬ì¿ ë¼': 'sakura',
            'íˆë‚˜íƒ€': 'hinata',
            'ì €ìŠ¹ì‚¬ì': 'reaper',
            'ê¹Œì¹˜': 'magpie',
            'í˜¸ë‘ì´': 'tiger'
        }
        
        # ì „ì²´ ë‹¨ì–´ ë§¤í•‘ í™•ì¸
        clean_text = re.sub(r'[^\wê°€-í£]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        if clean_text in word_map:
            return word_map[clean_text]
        
        # ê´„í˜¸ ì•ˆì˜ ì˜ì–´ ì¶”ì¶œ
        english_match = re.search(r'\(([A-Za-z\s]+)\)', text)
        if english_match:
            return re.sub(r'[^\w]', '_', english_match.group(1).strip())
        
        # ì˜ì–´ê°€ í¬í•¨ëœ ê²½ìš° ì˜ì–´ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        english_only = re.sub(r'[^A-Za-z0-9\s]', '', text)
        if english_only.strip():
            return re.sub(r'[^\w]', '_', english_only.strip())
        
        # í•œê¸€ì˜ ê²½ìš° ê¸°ë³¸ ë³€í™˜
        if re.search(r'[ê°€-í£]', text):
            return f'korean_{hash(text) % 10000}'
        
        return 'character'
    
    def download_and_upload_image(self, image_url: str, character_name: str) -> Optional[str]:
        """ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  S3ì— ì—…ë¡œë“œ"""
        try:
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = self.session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # ì´ë¯¸ì§€ ë°ì´í„° í™•ì¸
            if len(response.content) < 1000:  # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì œì™¸
                st.warning("ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤.")
                return None
            
            # íŒŒì¼ í™•ì¥ì ê²°ì •
            content_type = response.headers.get('content-type', '').lower()
            if 'jpeg' in content_type or 'jpg' in content_type:
                ext = '.jpg'
            elif 'png' in content_type:
                ext = '.png'
            elif 'webp' in content_type:
                ext = '.webp'
            else:
                # URLì—ì„œ í™•ì¥ì ì¶”ì¶œ ì‹œë„
                parsed_url = urlparse(image_url)
                path = parsed_url.path.lower()
                if '.jpg' in path or '.jpeg' in path:
                    ext = '.jpg'
                elif '.png' in path:
                    ext = '.png'
                elif '.webp' in path:
                    ext = '.webp'
                else:
                    ext = '.jpg'  # ê¸°ë³¸ê°’
            
            # S3 í‚¤ ìƒì„± (ìºë¦­í„° ì´ë¦„ì„ ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜)
            # í•œê¸€ì„ ë¡œë§ˆìë¡œ ë³€í™˜
            roman_name = self._korean_to_roman(character_name)
            
            # ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜ (ì˜ìˆ«ì, í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ë§Œ í—ˆìš©)
            safe_name = re.sub(r'[^\w\-_]', '_', roman_name).strip('_')
            
            # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
            safe_name = re.sub(r'_+', '_', safe_name)
            
            # ìµœëŒ€ ê¸¸ì´ ì œí•œ
            safe_name = safe_name[:50]
            
            # ë¹ˆ ë¬¸ìì—´ ë°©ì§€
            if not safe_name:
                safe_name = f'character_{int(time.time())}'
            
            s3_key = f"character-images/{safe_name}{ext}"
            
            # S3ì— ì—…ë¡œë“œ
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=s3_key,
                Body=response.content,
                ContentType=content_type or 'image/jpeg',
                Metadata={
                    'character_name_safe': safe_name,  # ASCII ì•ˆì „í•œ ì´ë¦„
                    'original_url': image_url,
                    'uploaded_at': str(int(time.time()))
                }
            )
            
            # S3 URL ìƒì„±
            s3_url = f"https://{self.bucket_name}.s3.amazonaws.com/{s3_key}"
            
            st.success(f"âœ… ì´ë¯¸ì§€ê°€ S3ì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {s3_key}")
            return s3_url
            
        except Exception as e:
            st.warning(f"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

    def _get_section_content(self, heading) -> str:
        """í—¤ë”© ë‹¤ìŒì˜ ë‚´ìš© ì¶”ì¶œ"""
        try:
            content = ""
            current = heading.next_sibling

            while current and current.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                if hasattr(current, 'get_text'):
                    content += current.get_text() + " "
                elif isinstance(current, str):
                    content += current + " "
                current = current.next_sibling

                if len(content) > 500:  # ë„ˆë¬´ ê¸¸ë©´ ì¤‘ë‹¨
                    break

            return content.strip()
        except Exception:
            return ""
    
    def auto_extract_character(self, character_name: str, use_bedrock_refinement: bool = True, extract_image: bool = True) -> Optional[Dict[str, Any]]:
        """ìºë¦­í„° ì´ë¦„ìœ¼ë¡œ ìë™ ì •ë³´ ì¶”ì¶œ (Bedrock ì •ì œ ì˜µì…˜ í¬í•¨)"""
        try:
            # 1ë‹¨ê³„: í˜ì´ì§€ URL ì°¾ê¸°
            with st.spinner(f"'{character_name}' ê²€ìƒ‰ ì¤‘..."):
                page_url = self.search_character(character_name)
                
            if not page_url:
                st.error(f"'{character_name}' í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            st.success(f"í˜ì´ì§€ ë°œê²¬: {page_url}")
            
            # 2ë‹¨ê³„: ì›ì‹œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            with st.spinner("í˜ì´ì§€ ë¶„ì„ ì¤‘..."):
                response = self.session.get(page_url, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë” ìœ ì—°í•œ ë°©ì‹)
                content_div = (
                    soup.find('div', {'class': 'wiki-content'}) or 
                    soup.find('div', {'id': 'content'}) or
                    soup.find('div', {'class': 'document'}) or
                    soup.find('div', {'class': 'wiki-inner-content'}) or
                    soup.find('main') or
                    soup.find('article')
                )
                
                if not content_div:
                    # ë§ˆì§€ë§‰ ì‹œë„: bodyì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    content_div = soup.find('body')
                
                if not content_div:
                    st.error("í˜ì´ì§€ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                
                raw_text = content_div.get_text()
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                raw_text = re.sub(r'\s+', ' ', raw_text)  # ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ
                raw_text = re.sub(r'[^\w\sê°€-í£.,!?()[\]{}:;"\'-]', '', raw_text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            
            # 3ë‹¨ê³„: ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì—…ë¡œë“œ (ì˜µì…˜)
            character_image_url = None
            if extract_image:
                with st.spinner("ğŸ–¼ï¸ ìºë¦­í„° ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘..."):
                    image_url = self.extract_character_image(soup, character_name)
                    if image_url:
                        st.info(f"ì´ë¯¸ì§€ ë°œê²¬: {image_url}")
                        character_image_url = self.download_and_upload_image(image_url, character_name)
                    else:
                        st.info("ì í•©í•œ ìºë¦­í„° ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # 4ë‹¨ê³„: Bedrockìœ¼ë¡œ ì •ë³´ ì •ì œ (ì˜µì…˜)
            if use_bedrock_refinement:
                with st.spinner("ğŸ¤– AIê°€ ì •ë³´ë¥¼ ì •ì œí•˜ëŠ” ì¤‘..."):
                    refined_info = self.refine_character_info_with_bedrock(raw_text, character_name)
                    
                    if refined_info:
                        # ì´ë¯¸ì§€ URL ì¶”ê°€
                        if character_image_url:
                            refined_info['image_url'] = character_image_url
                        
                        st.success("âœ¨ AI ì •ì œ ì™„ë£Œ! ê³ í’ˆì§ˆ ìºë¦­í„° ì •ë³´ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        return refined_info
                    else:
                        st.info("AI ì •ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # 4ë‹¨ê³„: ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ (fallback)
            with st.spinner("ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ ì¤‘..."):
                character_info = self.extract_character_info(page_url)
                
            if character_info:
                # ì´ë¯¸ì§€ URL ì¶”ê°€
                if character_image_url:
                    character_info['image_url'] = character_image_url
                
                # ë¹ˆ í•„ë“œ ê¸°ë³¸ê°’ ì„¤ì •
                if not character_info['role']:
                    character_info['role'] = f"{character_name}ì˜ ì—­í• "
                if not character_info['personality']:
                    character_info['personality'] = f"{character_name}ì˜ ë…íŠ¹í•œ ì„±ê²©"
                if not character_info['background']:
                    character_info['background'] = f"{character_name}ì˜ í¥ë¯¸ë¡œìš´ ë°°ê²½ ìŠ¤í† ë¦¬"
                if not character_info['catchphrase']:
                    character_info['catchphrase'] = f"{character_name}ì˜ ë©‹ì§„ ìºì¹˜í”„ë ˆì´ì¦ˆ!"
                if not character_info['speaking_style']:
                    character_info['speaking_style'] = f"{character_name}ë§Œì˜ ë…íŠ¹í•œ ë§íˆ¬"
                
                # ë¦¬ìŠ¤íŠ¸ ì¤‘ë³µ ì œê±°
                character_info['abilities'] = list(set(character_info['abilities']))[:10]  # ìµœëŒ€ 10ê°œ
                character_info['hobbies'] = list(set(character_info['hobbies']))[:10]  # ìµœëŒ€ 10ê°œ
                
                st.success("ì •ë³´ ì¶”ì¶œ ì™„ë£Œ!")
                return character_info
            else:
                st.error("ì •ë³´ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            st.error(f"ìë™ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    scraper = NamuWikiScraper()
    
    # í…ŒìŠ¤íŠ¸
    character_name = "í”¼ì¹´ì¸„"
    result = scraper.auto_extract_character(character_name)
    
    if result:
        print(json.dumps(result, ensure_ascii=False, indent=2))
